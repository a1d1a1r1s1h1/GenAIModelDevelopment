{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02bdaf04-c6e7-4661-8712-d5b2cb879271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: C:\\Users\\Adarsh\\venvs\\genai312\\Scripts\\python.exe\n",
      "Torch: 2.5.1+cu121\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import sys, torch\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49b90262-d62d-492b-9e37-0641b224037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 23767\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2461\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2891\n",
      "    })\n",
      "})\n",
      "{'text': ' = Valkyria Chronicles III = '}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Path where your files are (from your screenshot)\n",
    "data_dir = r\"C:\\Users\\Adarsh\\Downloads\\archive\\wikitext-2\"\n",
    "\n",
    "def load_text_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.read().split(\"\\n\")\n",
    "    # remove empty lines\n",
    "    lines = [l for l in lines if l.strip() != \"\"]\n",
    "    return Dataset.from_dict({\"text\": lines})\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": load_text_file(f\"{data_dir}/wiki.train.tokens\"),\n",
    "    \"validation\": load_text_file(f\"{data_dir}/wiki.valid.tokens\"),\n",
    "    \"test\": load_text_file(f\"{data_dir}/wiki.test.tokens\"),\n",
    "})\n",
    "\n",
    "print(ds)\n",
    "print(ds[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f7e7eb-e5c8-4156-8385-08a942777195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizers ready\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "gpt_tok  = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "bert_tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "t5_tok   = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "print(\"Tokenizers ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324921ea-4880-4952-b9fd-65cb53178e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT pad token: <|endoftext|> | id: 50256\n"
     ]
    }
   ],
   "source": [
    "# NEXT STEP: Set pad tokens (required for batching)\n",
    "gpt_tok.pad_token = gpt_tok.eos_token   # GPT-style uses EOS as PAD\n",
    "t5_tok.pad_token = t5_tok.pad_token     # already set, just explicit\n",
    "bert_tok.pad_token = bert_tok.pad_token # already set, just explicit\n",
    "\n",
    "print(\"GPT pad token:\", gpt_tok.pad_token, \"| id:\", gpt_tok.pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38af9241-5962-4713-9405-fff3c893763c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55070d985e074465b3e401a869bc11db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3cba082e01b49bf85bfd8665ea95f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d5fadf9f1f43fea4654be6c1c369df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a650b317daee49ca80caeb14c911e7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6af10e931746c09861941dcc3d8456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1c3cc9d5314a3a8744b2b6a85cb388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 23767\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2461\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2891\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# NEXT STEP: Tokenization for GPT (causal LM)\n",
    "\n",
    "def tokenize_gpt(examples):\n",
    "    return gpt_tok(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "gpt_ds = ds.map(tokenize_gpt, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# For GPT LM, labels = input_ids\n",
    "gpt_ds = gpt_ds.map(lambda x: {\"labels\": x[\"input_ids\"]}, batched=True)\n",
    "\n",
    "print(gpt_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b48b4c4c-d596-48ce-8ec0-48248a4f1724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4243261148df47918f1dbdd22d4f9a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd14f1b3e65a4d4e9d80c1987c302688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0259d380fb74bb5a151de0d4ade94fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 23767\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2461\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 2891\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# NEXT STEP: Tokenization for BERT (MLM)\n",
    "\n",
    "def tokenize_bert(examples):\n",
    "    return bert_tok(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "bert_ds = ds.map(tokenize_bert, batched=True, remove_columns=[\"text\"])\n",
    "print(bert_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "809a9825-360d-47b4-81bc-2065570eb9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8440b0764074a9386ae3895d4e26ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7163b995abfe4835888d73c86ae4bac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a005d12a9a4b14beb89b348c89be3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 23767\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2461\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2891\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# NEXT STEP: Tokenization for T5 using text_target (no as_target_tokenizer)\n",
    "\n",
    "def tokenize_t5(examples):\n",
    "    return t5_tok(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        text_target=examples[\"text\"],          # labels\n",
    "        max_target_length=128\n",
    "    )\n",
    "\n",
    "t5_ds = ds.map(tokenize_t5, batched=True, remove_columns=[\"text\"])\n",
    "print(t5_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78984a45-2c4f-4813-ab85-cb4b6e589259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b0c0d637fd4fbea5d5928037e4b5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1486' max='1486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1486/1486 03:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.406336</td>\n",
       "      <td>3.143550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT eval: {'eval_loss': 3.143549919128418, 'eval_runtime': 6.1609, 'eval_samples_per_second': 399.457, 'eval_steps_per_second': 24.996, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# NEXT STEP: Train GPT-style model (distilgpt2) on WikiText-2 (causal LM)\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# Causal LM model\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Data collator: causal LM (mlm=False)\n",
    "gpt_collator = DataCollatorForLanguageModeling(tokenizer=gpt_tok, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"runs_gpt\",\n",
    "    eval_strategy=\"epoch\",      # <-- new name in your version\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "gpt_trainer = Trainer(\n",
    "    model=gpt_model,\n",
    "    args=training_args,\n",
    "    train_dataset=gpt_ds[\"train\"],\n",
    "    eval_dataset=gpt_ds[\"validation\"],\n",
    "    data_collator=gpt_collator,\n",
    ")\n",
    "\n",
    "gpt_trainer.train()\n",
    "gpt_eval = gpt_trainer.evaluate()\n",
    "print(\"GPT eval:\", gpt_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b5908a-5e7b-4a6d-af83-38315df8d015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT validation perplexity: 23.186029466647234\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "gpt_ppl = math.exp(gpt_eval[\"eval_loss\"])\n",
    "print(\"GPT validation perplexity:\", gpt_ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f31175f0-dbb6-44e6-829c-05fba31f42ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e6e9fdcc52405b805321ef58cc155f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adarsh\\venvs\\genai312\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Adarsh\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9c0f69be8c4d6a80688d24e8506d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/202 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertForMaskedLM LOAD REPORT\u001b[0m from: bert-base-uncased\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "bert.pooler.dense.bias      | UNEXPECTED |  | \n",
      "cls.seq_relationship.bias   | UNEXPECTED |  | \n",
      "cls.seq_relationship.weight | UNEXPECTED |  | \n",
      "bert.pooler.dense.weight    | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1486' max='1486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1486/1486 03:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.635023</td>\n",
       "      <td>1.416378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT eval: {'eval_loss': 1.4000164270401, 'eval_runtime': 6.3316, 'eval_samples_per_second': 388.687, 'eval_steps_per_second': 24.323, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# NEXT STEP: Train BERT for Masked Language Modeling (MLM)\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# MLM collator (15% masking)\n",
    "bert_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=bert_tok,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "bert_args = TrainingArguments(\n",
    "    output_dir=\"runs_bert\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "bert_trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=bert_args,\n",
    "    train_dataset=bert_ds[\"train\"],\n",
    "    eval_dataset=bert_ds[\"validation\"],\n",
    "    data_collator=bert_collator,\n",
    ")\n",
    "\n",
    "bert_trainer.train()\n",
    "bert_eval = bert_trainer.evaluate()\n",
    "print(\"BERT eval:\", bert_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66e8d705-cf92-4425-8d01-8e7509a90ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT validation exp(MLM loss) (proxy): 4.05526658232429\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "bert_ppl_proxy = math.exp(bert_eval[\"eval_loss\"])\n",
    "print(\"BERT validation exp(MLM loss) (proxy):\", bert_ppl_proxy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6845fec4-82b3-4a64-ad42-49f0ca54526e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114c8eef1f604c5a855ac169392e7e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adarsh\\venvs\\genai312\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Adarsh\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336996fef29947ab81fd17d3ff9a8149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4c1fedc72e48a8b017f089eece5849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1486' max='1486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1486/1486 03:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.011877</td>\n",
       "      <td>0.000997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 eval: {'eval_loss': 0.0009973767446354032, 'eval_runtime': 7.1674, 'eval_samples_per_second': 343.362, 'eval_steps_per_second': 21.486, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# NEXT STEP: Train T5-small (seq2seq)\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "t5_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=t5_tok,\n",
    "    model=t5_model,\n",
    ")\n",
    "\n",
    "t5_args = TrainingArguments(\n",
    "    output_dir=\"runs_t5\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "t5_trainer = Trainer(\n",
    "    model=t5_model,\n",
    "    args=t5_args,\n",
    "    train_dataset=t5_ds[\"train\"],\n",
    "    eval_dataset=t5_ds[\"validation\"],\n",
    "    data_collator=t5_collator,\n",
    ")\n",
    "\n",
    "t5_trainer.train()\n",
    "t5_eval = t5_trainer.evaluate()\n",
    "print(\"T5 eval:\", t5_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee0262d8-52df-400b-8dd1-4779930feb07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2dad1451634a5282c26616f3116688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afa2054b96545d0962981146a43ff9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb0499c4cb64395a0685aedfdaac99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 23767\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2461\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2891\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# NEXT STEP: Build T5 denoising dataset (corrupt input, clean target)\n",
    "# Simple corruption: drop random tokens and ask T5 to reconstruct original text\n",
    "\n",
    "def corrupt_text(s, drop_prob=0.3):\n",
    "    toks = s.split()\n",
    "    if len(toks) < 8:\n",
    "        return s  # skip very short lines\n",
    "    kept = [t for t in toks if random.random() > drop_prob]\n",
    "    if len(kept) < 3:\n",
    "        kept = toks[:3]\n",
    "    return \" \".join(kept)\n",
    "\n",
    "def make_t5_denoise(examples):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for s in examples[\"text\"]:\n",
    "        s = s.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        inputs.append(\"denoise: \" + corrupt_text(s, drop_prob=0.35))\n",
    "        targets.append(s)\n",
    "\n",
    "    return t5_tok(\n",
    "        inputs,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        text_target=targets,\n",
    "        max_target_length=128\n",
    "    )\n",
    "\n",
    "t5_ds_denoise = ds.map(make_t5_denoise, batched=True, remove_columns=[\"text\"])\n",
    "print(t5_ds_denoise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20c2d46d-8dc1-4636-a72c-f41a2e1ad659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a45bd85c19428e8edee4589667cb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/131 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1486' max='1486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1486/1486 03:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.034427</td>\n",
       "      <td>0.944288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 denoise eval: {'eval_loss': 0.9442878365516663, 'eval_runtime': 6.6702, 'eval_samples_per_second': 368.954, 'eval_steps_per_second': 23.088, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# NEXT STEP: Retrain T5 on denoising objective\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "t5_model2 = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "t5_collator2 = DataCollatorForSeq2Seq(tokenizer=t5_tok, model=t5_model2)\n",
    "\n",
    "t5_args2 = TrainingArguments(\n",
    "    output_dir=\"runs_t5_denoise\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "t5_trainer2 = Trainer(\n",
    "    model=t5_model2,\n",
    "    args=t5_args2,\n",
    "    train_dataset=t5_ds_denoise[\"train\"],\n",
    "    eval_dataset=t5_ds_denoise[\"validation\"],\n",
    "    data_collator=t5_collator2,\n",
    ")\n",
    "\n",
    "t5_trainer2.train()\n",
    "t5_eval2 = t5_trainer2.evaluate()\n",
    "print(\"T5 denoise eval:\", t5_eval2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "213e1c9d-8470-4de9-92b0-eec96dfe19a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5 denoise validation perplexity: 2.5709817672214577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'GPT (distilgpt2) val_loss': 3.143549919128418,\n",
       " 'GPT perplexity': 23.186029466647234,\n",
       " 'BERT (MLM) val_loss': 1.4000164270401,\n",
       " 'BERT exp(MLM loss) proxy': 4.05526658232429,\n",
       " 'T5 (denoise) val_loss': 0.9442878365516663,\n",
       " 'T5 perplexity': 2.5709817672214577}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "t5_ppl = math.exp(t5_eval2[\"eval_loss\"])\n",
    "print(\"T5 denoise validation perplexity:\", t5_ppl)\n",
    "\n",
    "metrics_summary = {\n",
    "    \"GPT (distilgpt2) val_loss\": gpt_eval[\"eval_loss\"],\n",
    "    \"GPT perplexity\": gpt_ppl,\n",
    "    \"BERT (MLM) val_loss\": bert_eval[\"eval_loss\"],\n",
    "    \"BERT exp(MLM loss) proxy\": bert_ppl_proxy,\n",
    "    \"T5 (denoise) val_loss\": t5_eval2[\"eval_loss\"],\n",
    "    \"T5 perplexity\": t5_ppl,\n",
    "}\n",
    "\n",
    "metrics_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe2941d4-c791-45fc-a4ce-ce04dd5dde7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPT continuation ===\n",
      "Valkyria Chronicles III is the first game in the series to feature a three @-@ dimensional map featuring a single planet , as well as a playable boss . The game was developed by the team @-@ based on the original game and was developed by the company <unk> , and the game was released on 3DS\n",
      "\n",
      "=== BERT fill-mask (top 5) ===\n",
      "valkyria chronicles iii is a strategy game. | score: 0.5384\n",
      "valkyria chronicles iii is a video game. | score: 0.0586\n",
      "valkyria chronicles iii is a platform game. | score: 0.0364\n",
      "valkyria chronicles iii is a chess game. | score: 0.0231\n",
      "valkyria chronicles iii is a fantasy game. | score: 0.021\n",
      "\n",
      "=== T5 denoise ===\n",
      "Input : denoise: Valkyria Chronicles III is a tactical role playing game developed by Sega\n",
      "Output: Valkyria Chronicles III is a tactical role playing game developed by Sega.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ================= GPT (decoder-only) =================\n",
    "gpt_model = gpt_model.to(device)\n",
    "gpt_model.eval()\n",
    "\n",
    "prompt = \"Valkyria Chronicles III is\"\n",
    "gpt_inputs = gpt_tok(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    gpt_ids = gpt_model.generate(\n",
    "        **gpt_inputs,\n",
    "        max_new_tokens=60,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=gpt_tok.eos_token_id\n",
    "    )\n",
    "\n",
    "gpt_out = gpt_tok.decode(gpt_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n=== GPT continuation ===\")\n",
    "print(gpt_out)\n",
    "\n",
    "\n",
    "# ================= BERT (encoder-only, MLM) =================\n",
    "from transformers import pipeline\n",
    "\n",
    "bert_fill = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=bert_model,\n",
    "    tokenizer=bert_tok,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "masked = \"Valkyria Chronicles III is a [MASK] game.\"\n",
    "bert_out = bert_fill(masked, top_k=5)\n",
    "\n",
    "print(\"\\n=== BERT fill-mask (top 5) ===\")\n",
    "for o in bert_out:\n",
    "    print(o[\"sequence\"], \"| score:\", round(o[\"score\"], 4))\n",
    "\n",
    "\n",
    "# ================= T5 (encoder-decoder, denoise) =================\n",
    "t5_model2 = t5_model2.to(device)\n",
    "t5_model2.eval()\n",
    "\n",
    "corrupted = \"denoise: Valkyria Chronicles III is a tactical role playing game developed by Sega\"\n",
    "t5_inputs = t5_tok(corrupted, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    t5_ids = t5_model2.generate(\n",
    "        **t5_inputs,\n",
    "        max_new_tokens=60,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "t5_out = t5_tok.decode(t5_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n=== T5 denoise ===\")\n",
    "print(\"Input :\", corrupted)\n",
    "print(\"Output:\", t5_out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai312)",
   "language": "python",
   "name": "genai312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
