{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19da8a06-0def-43d7-9df2-546a79766253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a25555a5-9149-4897-a98c-9c7fe5b4f6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 23767\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2461\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2891\n",
      "    })\n",
      "})\n",
      "{'text': '= Valkyria Chronicles III ='}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data_dir = r\"C:\\Users\\Adarsh\\Downloads\\archive\\wikitext-2\"\n",
    "\n",
    "def load_text_file(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [l.strip() for l in f.read().split(\"\\n\") if l.strip()]\n",
    "    return Dataset.from_dict({\"text\": lines})\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": load_text_file(f\"{data_dir}/wiki.train.tokens\"),\n",
    "    \"validation\": load_text_file(f\"{data_dir}/wiki.valid.tokens\"),\n",
    "    \"test\": load_text_file(f\"{data_dir}/wiki.test.tokens\"),\n",
    "})\n",
    "\n",
    "print(ds)\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7848b3d-eb05-4ea7-ab29-6cc83c923d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816107de81b745dc9508a8c15fe3fd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model + tokenizer loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model + tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7406141a-fdb3-4079-8a15-6b1ced1bfd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ab093152a447d8944ba6c30d59fa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd13d8a043204167b3f116b070ea19ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31e9d0ff5a84f4594124c9897c848fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368a6e703b5c4fabbbb50ec6ccd997dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23767 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7efa58c1314118a77734ffc7d9ff23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e77174b5ee4cffaed3990d534c4da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 23767\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2461\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2891\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 128\n",
    "\n",
    "def tokenize_lm(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_ds = ds.map(tokenize_lm, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_ds = tokenized_ds.map(lambda x: {\"labels\": x[\"input_ids\"]}, batched=True)\n",
    "\n",
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4da0260-fd43-44f7-b9b9-efdbbf10bb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1486' max='1486' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1486/1486 03:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.438338</td>\n",
       "      <td>3.165188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1486, training_loss=3.529469671031187, metrics={'train_runtime': 196.3653, 'train_samples_per_second': 121.035, 'train_steps_per_second': 7.568, 'total_flos': 776279983915008.0, 'train_loss': 3.529469671031187, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import math\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pretrain_runs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1143186b-1f6d-4502-bdcf-a7019bf5b696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.1651883125305176\n",
      "Perplexity: 23.693205340211556\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "perplexity = math.exp(eval_results[\"eval_loss\"])\n",
    "\n",
    "print(\"Validation Loss:\", eval_results[\"eval_loss\"])\n",
    "print(\"Perplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e679fc2-2ff1-4c04-a628-5c4b99252421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6368c3965a4749b590bb650a2c2114be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pre-trained model to: model_pretrained\n"
     ]
    }
   ],
   "source": [
    "pretrain_dir = \"model_pretrained\"\n",
    "trainer.save_model(pretrain_dir)\n",
    "tokenizer.save_pretrained(pretrain_dir)\n",
    "print(\"Saved pre-trained model to:\", pretrain_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1178e0f4-b6b5-496f-887b-0f911cbc81a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'target'],\n",
      "        num_rows: 15339\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['prompt', 'target'],\n",
      "        num_rows: 1647\n",
      "    })\n",
      "})\n",
      "Example SFT item:\n",
      " {'prompt': 'Continue the text:\\nSenjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the', 'target': 'Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" .'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def make_pairs(lines, min_words=20):\n",
    "    pairs = []\n",
    "    for t in lines:\n",
    "        w = t.split()\n",
    "        if len(w) < min_words:\n",
    "            continue\n",
    "        mid = len(w) // 2\n",
    "        prompt = \"Continue the text:\\n\" + \" \".join(w[:mid])\n",
    "        target = \" \".join(w[mid:])\n",
    "        pairs.append({\"prompt\": prompt, \"target\": target})\n",
    "    return pairs\n",
    "\n",
    "train_pairs = make_pairs(ds[\"train\"][\"text\"])\n",
    "val_pairs   = make_pairs(ds[\"validation\"][\"text\"])\n",
    "\n",
    "sft_ds = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_pairs),\n",
    "    \"validation\": Dataset.from_list(val_pairs),\n",
    "})\n",
    "\n",
    "print(sft_ds)\n",
    "print(\"Example SFT item:\\n\", sft_ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef9c24ee-d11f-4add-bb18-72134e9c9e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faee67754e23437aa36ac0d642bdee90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69d8bdab2984243b7e5f8920149c74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 15339\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1647\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 256\n",
    "\n",
    "def tokenize_sft(batch):\n",
    "    prompts = batch[\"prompt\"]\n",
    "    targets = batch[\"target\"]\n",
    "\n",
    "    # full input = prompt + target\n",
    "    full_text = [p + \"\\n\" + t for p, t in zip(prompts, targets)]\n",
    "    full_enc = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # prompt token lengths (to mask labels)\n",
    "    prompt_enc = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i in range(len(prompts)):\n",
    "        lab = full_enc[\"input_ids\"][i].copy()\n",
    "\n",
    "        # compute number of non-pad tokens in prompt\n",
    "        prompt_len = 0\n",
    "        for tid in prompt_enc[\"input_ids\"][i]:\n",
    "            if tid == tokenizer.pad_token_id:\n",
    "                break\n",
    "            prompt_len += 1\n",
    "\n",
    "        # mask prompt portion\n",
    "        for j in range(min(prompt_len, len(lab))):\n",
    "            lab[j] = -100\n",
    "\n",
    "        labels.append(lab)\n",
    "\n",
    "    full_enc[\"labels\"] = labels\n",
    "    return full_enc\n",
    "\n",
    "sft_tok = sft_ds.map(tokenize_sft, batched=True, remove_columns=[\"prompt\", \"target\"])\n",
    "print(sft_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcfb4dad-bf32-455f-b25b-edd2c2890135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8397a16fb441edb3919ab03e4585be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='959' max='959' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [959/959 03:40, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.263554</td>\n",
       "      <td>3.053734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=959, training_loss=3.28565150033197, metrics={'train_runtime': 220.5873, 'train_samples_per_second': 69.537, 'train_steps_per_second': 4.347, 'total_flos': 1002007714332672.0, 'train_loss': 3.28565150033197, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import math\n",
    "\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(pretrain_dir)\n",
    "\n",
    "sft_args = TrainingArguments(\n",
    "    output_dir=\"sft_runs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "sft_trainer = Trainer(\n",
    "    model=sft_model,\n",
    "    args=sft_args,\n",
    "    train_dataset=sft_tok[\"train\"],\n",
    "    eval_dataset=sft_tok[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3299c1a5-0df4-4485-b25c-7463d0de53ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [103/103 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFT Validation Loss: 3.0537338256835938\n",
      "SFT Perplexity: 21.194332810531588\n"
     ]
    }
   ],
   "source": [
    "sft_eval = sft_trainer.evaluate()\n",
    "sft_ppl = math.exp(sft_eval[\"eval_loss\"])\n",
    "\n",
    "print(\"SFT Validation Loss:\", sft_eval[\"eval_loss\"])\n",
    "print(\"SFT Perplexity:\", sft_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e77c2a0e-005b-42b7-a0d1-939719739d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pre-training (LM)</td>\n",
       "      <td>3.165188</td>\n",
       "      <td>23.693205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SFT (prompt-&gt;target)</td>\n",
       "      <td>3.053734</td>\n",
       "      <td>21.194333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 method  val_loss  perplexity\n",
       "0     Pre-training (LM)  3.165188   23.693205\n",
       "1  SFT (prompt->target)  3.053734   21.194333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame([\n",
    "    {\"method\": \"Pre-training (LM)\", \"val_loss\": 3.1651883125305176, \"perplexity\": 23.693205340211556},\n",
    "    {\"method\": \"SFT (prompt->target)\", \"val_loss\": 3.0537338256835938, \"perplexity\": 21.194332810531588},\n",
    "])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9173932e-d823-4bd2-9f3d-e7c8ce3199e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ac7ce1c63545458cdb18a478ac22b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95235fb4417405a9159dd95c23de695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT:\n",
      " Continue the text:\n",
      "Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and\n",
      "\n",
      "GOLD TARGET (what SFT should learn to generate):\n",
      " bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into <unk> larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles .\n",
      "\n",
      "--- PRETRAIN OUTPUT ---\n",
      "\n",
      "Continue the text:\n",
      "Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and may have a diameter of 5 @.@ 1 – 7 mm ( 4 @.@ 2 – 6 @.@ 7 in ) . It is listed as the common lobster , and is listed as the common lobster . It has been described as a common lobster in the United States , Australia and New Zealand . It has been described as the common lobster in the United Kingdom , France and Spain , the\n",
      "\n",
      "--- SFT OUTPUT ---\n",
      "\n",
      "Continue the text:\n",
      "Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and is classified as the common lobster by the United States and Canada . It is a common and often misunderstood species . It is an <unk> @-@ related species in the United States . Its name was derived from the German word \" schmalthen \" and was first published in 1791 , and it is classified as the common lobster by the United States . It is a common\n",
      "Lob\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# reload models cleanly\n",
    "pre_model = AutoModelForCausalLM.from_pretrained(\"model_pretrained\").to(device)\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\"model_pretrained\").to(device)  # base\n",
    "sft_model.load_state_dict(sft_trainer.model.state_dict())  # updated SFT weights\n",
    "sft_model.to(device)\n",
    "\n",
    "def generate(model, prompt, max_new=80):\n",
    "    model.eval()\n",
    "    x = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        y = model.generate(\n",
    "            **x,\n",
    "            max_new_tokens=max_new,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.8,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    return tokenizer.decode(y[0], skip_special_tokens=True)\n",
    "\n",
    "# pick one prompt from validation set (same one used in SFT pairs)\n",
    "sample = sft_ds[\"validation\"][0]\n",
    "prompt = sample[\"prompt\"]\n",
    "gold   = sample[\"target\"]\n",
    "\n",
    "print(\"PROMPT:\\n\", prompt)\n",
    "print(\"\\nGOLD TARGET (what SFT should learn to generate):\\n\", gold)\n",
    "\n",
    "print(\"\\n--- PRETRAIN OUTPUT ---\\n\")\n",
    "print(generate(pre_model, prompt))\n",
    "\n",
    "print(\"\\n--- SFT OUTPUT ---\\n\")\n",
    "print(generate(sft_trainer.model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab8847fb-5871-4f9d-a485-1782bbef393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token overlap vs target (proxy)\n",
      "Pretrain: 0.2353\n",
      "SFT     : 0.2745\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def token_set_overlap(pred, ref):\n",
    "    pred_toks = set(re.findall(r\"\\w+\", pred.lower()))\n",
    "    ref_toks  = set(re.findall(r\"\\w+\", ref.lower()))\n",
    "    if len(ref_toks) == 0:\n",
    "        return 0.0\n",
    "    return len(pred_toks & ref_toks) / len(ref_toks)\n",
    "\n",
    "pre_out = generate(pre_model, prompt)\n",
    "sft_out = generate(sft_trainer.model, prompt)\n",
    "\n",
    "print(\"Token overlap vs target (proxy)\")\n",
    "print(\"Pretrain:\", round(token_set_overlap(pre_out, gold), 4))\n",
    "print(\"SFT     :\", round(token_set_overlap(sft_out, gold), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai312)",
   "language": "python",
   "name": "genai312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
